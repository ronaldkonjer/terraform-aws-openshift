# OpenShift Inventory Template.
# Note that when the infrastructure is generated by Terraform, this file is
# expanded into './inventory.cfg', based on the rules in:
#
#   ./modules/openshift/08-inventory.tf

# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes
etcd
nfs

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user=ec2-user

# If ansible_ssh_user is not root, ansible_become must be set to true
ansible_become=true

# Deploy OpenShift origin.
deployment_type=origin

openshift_clock_enabled=true

#openshift_deployment_type=origin
openshift_release=v3.9
openshift_image_tag="v3.9.0"
openshift_pkg_version="-3.9"

enable_excluders=false
containerized=true
openshift_use_system_containers=True
openshift_repos_enable_testing=True

# logging
openshift_logging_install_logging=true

# We need a wildcard DNS setup for our public access to services, fortunately
# we can use the superb xip.io to get one for free.
openshift_public_hostname=${public_hostname}
openshift_master_default_subdomain=${public_hostname}

openshift_master_identity_providers=[{'name': 'allow_all', 'login': 'true', 'challenge': 'true', 'kind': 'AllowAllPasswordIdentityProvider'}]

# Use an htpasswd file as the indentity provider.
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]
#openshift_master_htpasswd_users={'admin': '$apr1$w88LCwoM$V89slV1Fhl3IS2hN1byac.', 'developer': '$apr1$NhiYDJ9c$yNUYnB9RD0e6Ej73c8FIO1'

# LDAP auth
#openshift_master_identity_providers=[{'name': 'ldap_provider','challenge': 'true','login': 'true','kind': 'LDAPPasswordIdentityProvider','attributes': {'id': ['dn'],'email': ['mail'],'name': ['cn'],'preferredUsername': ['uid']},'bindDN': 'cn=Manager,dc=cgnet,dc=nl','bindPassword': 'J]Nd6dIS$]JD','insecure': 'true','url': 'ldap://zd005.cgnet.nl:389/cn=Manager,dc=cgnet,dc=nl?uid'}]


# OpenShift repository configuration
#openshift_additional_repos=[{'id': 'openshift-origin-copr', 'name': 'OpenShift Origin COPR', 'baseurl': 'https://copr-be.cloud.fedoraproject.org/results/maxamillion/origin-next/epel-7-$basearch/', 'enabled': 1, 'gpgcheck': 1, 'gpgkey': 'https://copr-be.cloud.fedoraproject.org/results/maxamillion/origin-next/pubkey.gpg'}]
#openshift_repos_enable_testing=true

# disable check on package version
#openshift_disable_check=package_version
openshift_disable_check='disk_availability,memory_availability,docker_storage,package_availability,package_version,docker_image_availability'

# Use API keys rather than instance roles so that tenant containers don't get
# Openshift's EC2/EBS permissions
openshift_cloudprovider_kind=aws
openshift_clusterid=${cluster_id}
openshift_cloudprovider_aws_access_key=${access_key}
openshift_cloudprovider_aws_secret_key=${secret_key}

# Docker additional external insecure repository
openshift_docker_additional_registries=docker-CG.cgnet.nl 
openshift_docker_insecure_registries=docker-CG.cgnet.nl 

# Upgrade Hooks
#
# Hooks are available to run custom tasks at various points during a cluster
# upgrade. Each hook should point to a file with Ansible tasks defined. Suggest using
# absolute paths, if not the path will be treated as relative to the file where the
# hook is actually used.

# Define an additional dnsmasq.conf file to deploy to /etc/dnsmasq.d/openshift-ansible.conf
# This is useful for POC environments where DNS may not actually be available yet or to set
# options like 'strict-order' to alter dnsmasq configuration.
openshift_node_dnsmasq_additional_config_file=${dnsmasq_conf}

# additional cors origins
osm_custom_cors_origins=['sc.*','s3.*']


# Metrics deployment
# See: https://docs.openshift.com/enterprise/latest/install_config/cluster_metrics.html
# Uncomment the line below to enable metrics for the cluster.
openshift_metrics_install_metrics=true
openshift_metrics_cassandra_storage_type=dynamic
#openshift_metrics_cassandra_storage_type=pv


# Storage Options
# If openshift_metrics_storage_kind is unset then metrics will be stored
# in an EmptyDir volume and will be deleted when the cassandra pod terminates.
# Storage options A & B currently support only one cassandra pod which is
# generally enough for up to 1000 pods. Additional volumes can be created
# manually after the fact and metrics scaled per the docs.
#
# Option A - NFS Host Group
# An NFS volume will be created with path "nfs_directory/volume_name"
# on the host within the [nfs] host group.  For example, the volume
# path using these options would be "/exports/metrics".  "exports" is
# is the name of the export served by the nfs server.  "metrics" is
# the name of a directory inside of "/exports".
openshift_metrics_storage_kind=nfs
openshift_metrics_storage_access_modes=['ReadWriteOnce']
openshift_metrics_storage_nfs_directory=/exports
openshift_metrics_storage_nfs_options='*(rw,root_squash)'
openshift_metrics_storage_volume_name=metrics
openshift_metrics_storage_volume_size=10Gi
#openshift_metrics_storage_labels={'storage': 'metrics'}

#openshift_metrics_heapster_allowed_users ["system:master-proxy", "system:admin"]

### CONFIGURING PERSISTENT STORAGE FOR THE OPENSHIFT ANSIBLE BROKER
openshift_hosted_etcd_storage_kind=nfs
openshift_hosted_etcd_storage_nfs_options="*(rw,root_squash,sync,no_wdelay)"
openshift_hosted_etcd_storage_nfs_directory=/opt/osev3-etcd 
openshift_hosted_etcd_storage_volume_name=etcd-vol2 
openshift_hosted_etcd_storage_access_modes=["ReadWriteOnce"]
openshift_hosted_etcd_storage_volume_size=1G
#openshift_hosted_etcd_storage_labels={'storage': 'etcd'}

# Prometheus deployment
#
# Currently prometheus deployment is disabled by default, enable it by setting this
openshift_hosted_prometheus_deploy=true
openshift_prometheus_namespace=openshift-metrics
openshift_prometheus_node_selector={"region": "infra"}
      

# Prometheus storage config
# Option A - NFS Host Group
# An NFS volume will be created with path "nfs_directory/volume_name"
# on the host within the [nfs] host group.  For example, the volume

# path using these options would be "/exports/prometheus"
openshift_prometheus_storage_kind=nfs
openshift_prometheus_storage_access_modes=['ReadWriteOnce']
openshift_prometheus_storage_nfs_directory=/exports
openshift_prometheus_storage_nfs_options='*(rw,root_squash)'
openshift_prometheus_storage_volume_name=prometheus
openshift_prometheus_storage_volume_size=10Gi
#openshift_prometheus_storage_labels={'storage': 'prometheus'}
openshift_prometheus_storage_type='pvc'
openshift_prometheus_storage_class='gp2'

# For prometheus-alertmanager
openshift_prometheus_alertmanager_storage_kind=nfs
openshift_prometheus_alertmanager_storage_access_modes=['ReadWriteOnce']
openshift_prometheus_alertmanager_storage_nfs_directory=/exports
openshift_prometheus_alertmanager_storage_nfs_options='*(rw,root_squash)'
openshift_prometheus_alertmanager_storage_volume_name=prometheus-alertmanager
openshift_prometheus_alertmanager_storage_volume_size=10Gi
#openshift_prometheus_alertmanager_storage_labels={'storage': 'prometheus-alertmanager'}
openshift_prometheus_alertmanager_storage_type='pvc'
openshift_prometheus_alertmanager_storage_class='gp2'

# For prometheus-alertbuffer
openshift_prometheus_alertbuffer_storage_kind=nfs
openshift_prometheus_alertbuffer_storage_access_modes=['ReadWriteOnce']
openshift_prometheus_alertbuffer_storage_nfs_directory=/exports
openshift_prometheus_alertbuffer_storage_nfs_options='*(rw,root_squash)'
openshift_prometheus_alertbuffer_storage_volume_name=prometheus-alertbuffer
openshift_prometheus_alertbuffer_storage_volume_size=10Gi
#openshift_prometheus_alertbuffer_storage_labels={'storage': 'prometheus-alertbuffer'}
openshift_prometheus_alertbuffer_storage_type='pvc'
openshift_prometheus_alertbuffer_storage_class='gp2'


# Currently logging deployment is disabled by default, enable it by setting this
#openshift_logging_install_logging=true
#
# Logging storage config
openshift_logging_storage_kind=dynamic

# Create the masters host group. Note that due do:
#   https://github.com/dwmkerr/terraform-aws-openshift/issues/40
# We cannot use the internal DNS names (such as master.openshift.local) as there
# is a bug with the installer when using the AWS cloud provider.
[masters]
${master_hostname} openshift_hostname=${master_hostname}

# host group for etcd
[etcd]
${master_hostname} openshift_hostname=${master_hostname}

[nfs]
${master_hostname} openshift_hostname=${master_hostname}

# host group for nodes, includes region info
[nodes]
${master_hostname} openshift_hostname=${master_hostname} openshift_node_labels="{'region': 'infra', 'zone': 'default'}" openshift_schedulable=true
${node1_hostname} openshift_hostname=${node1_hostname} openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
${node2_hostname} openshift_hostname=${node2_hostname} openshift_node_labels="{'region': 'primary', 'zone': 'west'}"


